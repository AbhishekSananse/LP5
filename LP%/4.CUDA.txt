!nvcc --version

!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git

%load_ext nvcc_plugin

%%cu
#include <cuda_runtime.h>
#include <iostream>
__global__ void matmul(int* A, int* B, int* C, int N) {
    int Row = blockIdx.y*blockDim.y+threadIdx.y;
    int Col = blockIdx.x*blockDim.x+threadIdx.x;
    if (Row < N && Col < N) {
        int Pvalue = 0;
        for (int k = 0; k < N; k++) {
            Pvalue += A[Row*N+k] * B[k*N+Col];
        }
        C[Row*N+Col] = Pvalue;
    }
}
int main() {
    int N = 512;
    int size = N * N * sizeof(int);
    int* A, * B, * C;
    int* dev_A, * dev_B, * dev_C;
    cudaMallocHost(&A, size);
    cudaMallocHost(&B, size);
    cudaMallocHost(&C, size);
    cudaMalloc(&dev_A, size);
    cudaMalloc(&dev_B, size);
    cudaMalloc(&dev_C, size);
    // Initialize matrices A and B
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            A[i*N+j] = i*N+j;
            B[i*N+j] = j*N+i;
        }
    }
    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);
    dim3 dimBlock(16, 16);
    dim3 dimGrid(N/dimBlock.x, N/dimBlock.y);
    matmul<<<dimGrid, dimBlock>>>(dev_A, dev_B, dev_C, N);
    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);
    // Print the result
    for (int i = 0; i < 10; i++) {
        for (int j = 0; j < 10; j++) {
            std::cout << C[i*N+j] << " ";
        }
        std::cout << std::endl;
    }
    // Free memory
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaFree(dev_C);
    cudaFreeHost(A);
    cudaFreeHost(B);
    cudaFreeHost(C);
    return 0;
}

%%cu
#include <iostream>  
#include <cuda_runtime.h>
using namespace std;
__global__ void addVectors(int* A, int* B, int* C, int n) 
{
    int i = blockIdx.x * blockDim.x + threadIdx.x; 
    if (i < n) 
    {
        C[i] = A[i] + B[i];
    }
}
int main() 
{
    int n = 1000000;  
    int* A, * B, * C;
    int size = n * sizeof(int);
    // Allocate memory on the host  
    cudaMallocHost(&A, size);  
    cudaMallocHost(&B, size);  
    cudaMallocHost(&C, size);
    // Initialize the vectors
    for (int i = 0; i < n; i++) 
    {
        A[i] = i;
        B[i] = i * 2;
    }
    // Allocate memory on the device  
    int* dev_A, * dev_B, * dev_C;  
    cudaMalloc(&dev_A, size);  
    cudaMalloc(&dev_B, size);  
    cudaMalloc(&dev_C, size);
    // Copy data from host to device
    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);  
    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);
    // Launch the kernel  
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n);
    // Copy data from device to host
    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);
    // Print the results
    for (int i = 0; i < 10; i++) 
    {
        cout << C[i] << " ";
    }
    cout << endl;
    // Free memory  
    cudaFree(dev_A);  
    cudaFree(dev_B);  
    cudaFree(dev_C);  
    cudaFreeHost(A);  
    cudaFreeHost(B);  
    cudaFreeHost(C);
    return 0;
}






The provided code demonstrates two CUDA C++ programs using the CUDA runtime API. 
These programs perform matrix multiplication and vector addition on the GPU. 
Here's a breakdown of each program:

1. Matrix Multiplication Program (`matmul`):

- The program defines a CUDA kernel function `matmul` that performs 
matrix multiplication of two square matrices.
- The kernel takes pointers to matrices `A`, `B`, and `C`, as well as 
the matrix size `N` as input.
- Each thread calculates an element of the resulting matrix `C` by iterating 
over the corresponding row and column in matrices `A` and `B`.
- The program's main function initializes host and device memory, initializes 
matrices `A` and `B`, transfers data from the host to the device, launches the 
`matmul` kernel using a grid of thread blocks, transfers the result from the 
device to the host, and prints the first 10x10 elements of matrix `C`.
- Finally, the program frees allocated memory on both the host and the device.

2. Vector Addition Program (`addVectors`):

- The program defines a CUDA kernel function `addVectors` that performs element-wise
 addition of two input vectors `A` and `B` and stores the result in vector `C`.
- The kernel takes pointers to vectors `A`, `B`, and `C`, as well as the vector size `n` as input.
- Each thread calculates the sum of the corresponding elements from vectors `A` 
and `B` and stores the result in vector `C`.
- The program's main function initializes host and device memory, initializes vectors `A` and `B`,
transfers data from the host to the device, launches the `addVectors` kernel using a grid of 
thread blocks, transfers the result from the device to the host, and prints the first 10 elements
 of vector `C`.
- Finally, the program frees allocated memory on both the host and the device.

To run these CUDA programs in a Jupyter Notebook, you need to have an NVIDIA GPU and
 the CUDA toolkit properly installed and configured on your system. 
Additionally, you need to have the `nvcc4jupyter` package installed to enable CUDA 
compilation and execution in Jupyter Notebook.